# Preamble

```{r}
#| label: load-libs
#| message: false
#| warning: false
#| results: hide
source("utils.R")
theme_set(theme_bw())
```

## Setup

```{r, message=FALSE}
#| label: load-data
spe = readRDS("../data/spe.rds")

#subset the data to only look at sample ID 0.01
sub = spe[, spe$sample_id == 0.01]
#sub = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte", "Pericytes")]
(pp = .ppp(sub, marks = "cluster_id"))


#subset the data to only look at sample ID 0.01
sub_2CT = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte", "Inhibitory")]
#sub = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte", "Pericytes")]
(pp_2CT = .ppp(sub_2CT, marks = "cluster_id"))

#split the multitype point process into several single type processes
#fist, set the marks of the point process to be factors
marks(pp) = factor(marks(pp))
ppls = split(pp)
```

```{r, fig.height=15, fig.width=15}
plot(ppls)
```

# Point pattern approach

In `spatstat` a `mark` can basically take any value. It can be discrete as we have seen in the discrete mark chapter or it can take a continuos value, such as gene expression. In our example we take the gene expression of some marker genes from Fig. 6 of the original publication.

```{r}
#  Genes from Fig. 6 of Moffitt et al. (2018)
gex <- as.data.frame(t(as.matrix(assay(sub))))[, c('Slc18a2', 'Esr1', 'Pgr')]
rownames(gex) <- NULL
# gene expression to marks
marks(pp) <- gex
```

> TODO: better plotting?

```{r}
plot(pp)
```

A pairs plot indicates spatial inhomogeneity, i.e. a spatial trend of the of the marks. This plot is only reasonable for a small number of marks however.

```{r}
pairs(as.data.frame(pp), panel = panel.smooth)
```

The `Smooth` command uses cross-validation to select the smoothing bandwidth of the Gaussian kernel. This estimated kernel can be used for visual inspection of the dataset.

```{r}
ppsmooth <- Smooth(pp, bw.smoothppp)
plot(ppsmooth)
```
From the estimated intesity plot we can see that the expression of the marker genes is clearly inhomogeneous.

Nearest-neighbour interpolations uses the nearest mark to measure the intensity at each spatial location. This is conceptually similar to taking a very small bandwith for the Gaussian kernel.

```{r}
plot(nnmark(pp))
```
We can use the average value of neighbouring point to predict the expression of a gene at each point. We can then plot the actual marks versus the fitted values to detect anomalies. E. g. `Esr1` shows a clear half moon shape in the middle of the image, where the actual values are much higher than the fitted. This gives further indication of structure in the gene expression.

```{r}
mfit <- Smooth(pp, bw.smoothppp, at="points")
res <- marks(pp) - mfit

plot(setmarks(pp, res))
```

## Summary functions

As in the discrete case the summary functions assume that the point process is stationary.

### Mark correlation function

The mark correlation function measures the dependence between the marks at two points at distance $r$. It is not a correlation in the classical sense, since it can take any nonegative value. The value of 1 indicates no correlation between the marks. The generalized mark correlation function is given by

$$ k_f(r) = \frac{\mathbb{E}[f(m(u)m(v))]}{\mathbb{E}[f(M,M')]}$$
where $f(m_1,m_2)$ is a test function with two arguments that represent the marks and returns a non-negative value. For continuous non-negative marks the choice of $f$ is by default $f(m_1,m_2)= m_1 m_2$. $M, M′$ represent independent, identically distributed random points with same distribution as the mark of a randomly chosen point. This denominator is chosen such that random marks have a mark correlation of 1.

```{r}
plot(markcorr(pp))
```
We can compare the mark correlation function to a pointwise simulation envelope in which we generate 5 simulations of random labeling.  
```{r}
ppEsr1 <- subset(pp, select = 'Esr1')
markcorr.Esr1 <- envelope(ppEsr1, markcorr, nsim=10)

plot(markcorr.Esr1)
```

The plot indicates that the mark correlation function is significantly different from the random case. The positive association of expression of the Esr1 gene declines with distance. This is consistent with clustering we saw in the residual plot above. 

<!-- We can calculate the cross correlation function between the different gene expression measurments, the diagonal is equivalent to `markcorr` -->

<!-- ```{r} -->
<!-- mc <- markcrosscorr(pp) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- par(mar = c(1, 1, 1, 1)) -->
<!-- plot(mc) -->
<!-- ``` -->

# Mark-weigthed K function

Mark-weigthed K function is a generalization of the K-function in which the contribution of each from each pair of points is weighted by a function of their respective marks. The mark-weighted K-function is given by

$$K_f(r) = \frac 1  \lambda \frac{C_f(r)}{E[ f(M_1, M_2) ]}$$
with the function $C_f(r) = E \left[ \sum_{x \in X} f(m(u), m(x)) 1{0 < ||u - x|| \le r} \;  \big| \; u \in X \right]$ which is equivalent to the unnormalized mark-weighted $K$-function. For every point $u$ we sum the euclidean distance $||u - x||$ of all other points $x$ that are within a distance $r$. This sum is weighted by the function $f$ of the marks of $u$ and $x$. The function is standardized by the expected value of $f(M, M′)$ where $M, M′$ represent independent, identically distributed random points with same distribution as the mark of a randomly chosen point.

In the scenario of random labeling, the mark-weighted $K$-function corresponds to Ripley's $K$-function.    

The default function is $f(m_1, m_2) = m_1 m_2$. 

```{r}
ppEsr1 <- subset(pp, select = 'Esr1')

K.Esr1L <- Kmark(ppEsr1, function(m1,m2) {m1*m2})
plot(K.Esr1L)
```
It is important to note that theoretical value is not very informative since it represents the $K$-function of a Poisson point process and the underlying point process might not be Poisson. Therefore we compare the mark-weighted with its unmarked analogue. 

Here we will compare the $L$-functions, which are the variance stabilized $K$-functions.

```{r}
L.Esr1L <- Kmark(ppEsr1, function(m1,m2) {m1*m2}, returnL = TRUE)
Lest.ppEsr1 <- Lest(ppEsr1, nlarge=7000)
plot(eval.fv(L.Esr1L - Lest.ppEsr1))
```
Like for other functions we can calculate a simulation envelope for the mark-weighted K-function to get confidence intervals.

```{r}
plot(envelope(ppEsr1, fun=Kmark, returnL=TRUE, nsim=50))
```

# Lattice based approach

Until now we have considered the cells to be represented in a point pattern. However, as cells have a shape and area, this might be an oversimplification in some cases. Alternatively, we can rely on the segmentation of indvidual cells that are available for various datasets. The outline of each cell is represented by a polygon and the collection of all cells can be seen as an irregular lattice. Unlike a regular lattice (the representation of spot based spatial tanscriptomics data) the sample areas in an irregular lattice can have different sizes and are not reugularly distributed over the sample space.

For this representation of the cells we will rely on the `SpatialFeatureExperiment` package. For preprocessing of the dataset we refer the reader to the [vignette](https://pachterlab.github.io/voyager/articles/vig4_cosmx.html) of the `voyager` package.

```{r}
(sfe <- HeNSCLCData())

# Empty cells
colData(sfe)$is_empty <- colData(sfe)$nCounts < 1
# Select negative control probes
neg_inds <- str_detect(rownames(sfe), "^NegPrb")
# Number of negative control probes
sum(neg_inds)
colData(sfe)$prop_neg <- colSums(counts(sfe)[neg_inds,])/colData(sfe)$nCounts
# Remove low quality cells
(sfe <- sfe[,!sfe$is_empty & sfe$prop_neg < 0.1])
# Calculate count stats
rowData(sfe)$means <- rowMeans(counts(sfe))
rowData(sfe)$vars <- rowVars(counts(sfe))
rowData(sfe)$is_neg <- neg_inds
# log Counts
sfe <- logNormCounts(sfe)
```

```{r}
plotSpatialFeature(sfe, c("nGenes", "nCounts"),
                   colGeometryName = "centroids", ncol = 2, scattermore = TRUE)
```

## Irregular lattice and neighbourhood matrix

Most analysis techniques rely on the a neighborhood matrix, which is a matrix that indicates which cells are neighbors. In the case of the regular lattice, the calculation of the neighborhood matrix is rather straightforward. In the case of the irregular lattice it can be more complicated as the reconstruction of the cells is often not perfect.

## Neighbourhood matrix

```{r}
colGraph(sfe, "knn5") <- findSpatialNeighbors(sfe, method = "knearneigh",
                                                  dist_type = "idw", k = 5, 
                                                  style = "W")
```

# Global Moran's I coefficient

The global Moran's I coefficient is a measure of spatial autocorrelation. It is defined as

$$I = \frac{n}{\sum_i\sum_j w_{ij}} \frac{\sum_i\sum_j w_{ij}(y_i - \hat{y})(y_j - \hat{y})}{\sum_i (y_i - \hat{y})^2}$$

where $y_i$ is the value of the variable of interest for location $i$, $\hat{y}$ is the mean of $y_i$ and $w_{ij}$ is the spatial weight between locations $i$ and $j$. This value is close to $0$ for large $n$. A value higher than $\mathbb{E}(I) = -1/(n-1)$ indicates spatial auto-correlation. Negative values indicate negative auto-correlation.

## Implementation using `voyager`

```{r}
features_use <- c("nGenes")
sfe <- colDataMoransI(sfe, features_use, colGraphName = "knn5")

colFeatureData(sfe)[features_use,]

sfe <- colDataUnivariate(sfe, features = c("nCounts", "nGenes"), 
                                colGraphName = "knn5", nsim = 200,
                                type = "moran.mc")
res <- colFeatureData(sfe)[c("nCounts", "nGenes"),]
#value of the metric
res$moran.mc_statistic_sample01
#p-value
res$moran.mc_p.value_sample01
```

## Implementation using `spdep`

```{r}
#create nearest neighbours weights 
weights_neighbourhoods <- colGraph(sfe, "knn5")
spdep::moran.test(x = sfe$nGenes, listw = weights_neighbourhoods, randomisation = FALSE)
spdep::moran.test(x = sfe$nCounts, listw = weights_neighbourhoods, randomisation = FALSE)
```

# Global's Geary's coefficient

Geary's $C$ is a different measure of global autocorrelation and is very closely related to Moran's $I$. Geary's $C$ is defined by

$$
C = \frac{(n-1) \sum_i \sum_j w_{ij}(y_i-y_j)^2}{2\sum_i \sum_j w_{ij}\sum_i(y_i-\bar{y})^2}
$$
where $n$ is the number of locations, $i,j$ are different locations, $y_i$ is the value of the variable of interest for location $i$, $\bar{y}$ is the mean of $y_i$ and $w_{ij}$ is the spatial weight between locations $i$ and $j$.  The interpretation is inversely to Moran's $I$. A value less than $1$ indicates positive auto-correlation, a value more than $1$ negative auto-correlation.


## Implementation using `voyager`

```{r}
features_use <- c("nGenes")
sfe <- colDataMoransI(sfe, features_use, colGraphName = "knn5")

colFeatureData(sfe)[features_use,]

sfe <- colDataUnivariate(sfe, features = c("nCounts", "nGenes"), 
                                colGraphName = "knn5", nsim = 200,
                                type = "geary.mc")
res <- colFeatureData(sfe)[c("nCounts", "nGenes"),]
#value of the metric
res$geary.mc_statistic_sample01
#p-value
res$geary.mc_p.value_sample01
```
## Implementation using `spdep`

```{r}
#create nearest neighbours weights 
weights_neighbourhoods <- colGraph(sfe, "knn5")
spdep::geary.test(x = sfe$nGenes, listw = weights_neighbourhoods, randomisation = TRUE)
spdep::geary.test(x = sfe$nCounts, listw = weights_neighbourhoods, randomisation = TRUE)
```



# OLD UNREFERENCED CODE

# Local Indicators of Spatial Association–LISA

## Bivariate Global Moran's I

Global morans'I concept is equal to local but one can plot continuous variable on x axis and spatial lag on y axis and the slope is equal to morans I value. With 4 quadrants that allow us to understand the global picture of spatial correlation**

**While the local one gives us information about more local environments**

$B$

**Concept**
-> Moran's I is a correlation coefficient that measures the **overall spatial autocorrelation** of a dataset.

-> Moran's I values ranging from –1 to 1. 

  + -1 is + perfect clustering of dissimilar values (perfect dispersion) 
  + 0 is no autocorrelation (perfect randomness) 
  + 1 indicates perfect clustering (opposite of perfect dispersion) 

-> Moran's I **output cannot be taken directly for interpretation**. It is an inferential statistic, and one has to determine statistical significance by using **hypothesis test**, calculating z-score and its associated p-value. 

  + The **null hypothesis** for the test is that the data is randomly distributed. 
  + The **alternative hypothesis** is that the data is more spatially clustered than you would expect by chance alone. 2 scenarios: 
    - Positive z-score: data is spatially clustered in some way. 
    - Negative z-score: data is clustered in a competitive way. For example, high values may be repelling high values or negative values may be repelling negative values in a competitive way. 
    
**Calculations** 

-> Calculations for Moran's I are based on a weighted matrix, with units i and j. Similarities between units is calculated as the product of the differences between yi and yj with the overall mean. 

$$ 
similarity = (y_i - \overline{y}) * (y_j - \overline{y})
\\where \ \overline{y} = \sum_{i = 1}^{n} y_i / n 
$$

-> Moran's statistic is calculated using the basic form, which is divided by s (sample variance)

$$
I = \frac{\sum_{i} \sum_{j} (y_i - \overline{y}) * (y_j - \overline{y}) }{\sum_{i} \sum_{j} w_{ij}} * \frac{1}{S²}
\\S² = \frac{\sum_{(y_i - \overline{y})²}}{n}
$$

wij as the elements of the spatial weights matrix

N is the number of observations

-> The distribution of the statistic under the null can be derived using either an **assumption of normality** or so-called **randomization**. While the analytical derivations provide easy way to interpret the mean and the variance of the statistic under the null hypothesis, inference based on them employs an approximation to a standard normal distribution, which may be inappropriate when the underlying assumptions are not satisfied. 

-> An alternative to the analytical derivation is a computation approach based on **permutation**. This calculates a reference distribution for the statistic under the null hypothesis of spatial randomnes by **randomly permuting the observed values over the locations**. The statistic is computed for each of these randomly reshuffled data sets, which yields a **reference distribution**. 

-> The reference distribution is used to calculate so-called pseudo p-value as follows: 


$$
p=\frac{R + 1}{M + 1} 
$$

Where R is the number of times the computed Moran's I from the spatial random datasets is equal to or more extreme than the observed statistic.
M equals the number of permutations. 
 
Generate spatial weights (The weight matrix is often row-standardized - all the wieghts in a row sum to 1)

  + Contiguity Based Weights: `queen_weights()`, `rook_weights()`
  + Distance Based Weights: `distance_weights()`
  + K-Nearest Neighbor Weights: `knn_weights()`
  + Kernel Weights: `distance_weights()` and `knn_weights()` with kernel parameters

### Morans'I scatter plot

-> Plot with the spatially lagged variable on the y-axis and the original variable on the x-axis. The **slope of the linear fit to the scatter plot equals Moran’s I value**.

-> An important aspect of the visualization in the Moran scatter plot is the classification of the nature of spatial autocorrelation into four categories. Since the plot is centered on the mean (of zero), all points to the right of the mean have zi>0 and all points to the left have zi<0 .We refer to these values respectively as high and low, in the limited sense of higher or lower than average. Similarly, we can classify the values for the spatial lag above and below the mean as high and low.

-> as such, we have 4 quadrants:
  + (>0 >0 -> High-high) | (<0 <0 -> Low-low) Correspond to positive spatial correlation (Similar values at the neighborhood locations)
  + (<0 >0 -> Low-high) | (>0 <0 -> High-low) Correspond to negative spatial correlation (Dissimilar values at the neighborhood locations)

-> Spatial lag is calculated on standardized values of the variable of interest and thus, the plot is in x axis (standardized values of variable of interest) and y axis (spatial lag)

-> A **spatial lag** of a specified variable is computed by taking the **weighted average of neighboring polygons**, as specified in the weights matrix. For example: a census tract with three neighboring tracts that have 10%, 15%, and 20% black population would have a spatial lag of 15%; that is, [(10%+15%+20%)/3]. The row-standardized spatial weights matrix is used in the calculation of the spatial lag. 

Calculate spatial lag
```{r,eval=FALSE}
sptl_lag = spatial_lag(knn_weights_sfObj, sfObj["mark"])

sptl_lag %>% as.vector() %>% unlist %>% hist
```

**I am not sure how to represent non-numerical variables for the morans plot**

## Local Moran's I

-> With row-standardized weights, the sum of all weights:

$$
S_0 = \sum_{i}\sum_{j}w_{ij}
$$
equals the number of observations, n. As a result, Moran's statistic simplifies to:
$$
I = \frac{\sum_{i}\sum_{j} w_{ij} z_iz_j}{\sum_{i}z²}
$$
where z is the deviations from the mean. This can be simplified to:
$$
I_i = c.z_i \sum_{j} w_{ij}z_j
$$
since the dominator of previous equation is constant, it is replaced by c for simplicity...

The equation represents product of the value at location i with its spatial lag, the weighted sum of the values at neighboring locations. A little bit of algebra shows that the sum of the local statistics is proportional to the global Moran’s I, or, alternatively, that the global Moran’s I corresponds with the average of the local statistics
 
-> Significance can be based on an analytical approximation, but, as argued in Anselin (1995), this is not very reliable in practice. A preferred approach consists of a **conditional permutation method**. This is similar to the permutation approach mentioned above, except that the value of each zi is held fixed at its location i. The remaining n-1 z-values are then randomly permuted to yield a reference distribution for the local statistic (one for each location).


```{r}
# convert ppp object to sf object
sfObj = st_as_sf(pp)

# Remove 1st row -> window info (otherwise crash)
sfObj = sfObj[-1,]

# change name of 1st column
colnames(sfObj)[1] = c("mark")

# convert marks to factors
sfObj$mark = as.factor(sfObj$mark)

# calculate weights based on knn -> others algos can be used
knn_weights_sfObj = knn_weights(sfObj, k = sqrt(dim(sfObj)[1]) %>% floor) # 
```

Moran's test
```{r}
lisa = local_moran(knn_weights_sfObj, sfObj["mark"], cpu_threads = parallel::detectCores() * 0.75)

#lisa_clusters(): Get the local cluster indicators returned from LISA computation.
#lisa_colors(): Get the cluster colors of LISA computation.
#lisa_labels(): Get the cluster labels of LISA computation.
#lisa_values(): Get the local spatial autocorrelation values returned from LISA computation.
#lisa_num_nbrs(): Get the number of neighbors of every observations in LISA computation.
#lisa_pvalues(): Get the local pseudo-p values of significance returned from LISA computation.
#lisa_fdr(): Get the False Discovery Rate (FDR) in LISA.
#lisa_bo(): Get the False Discovery Rate (FDR) in LISA.
```

Get the False Discovery Rate value based on current pseudo-p values:
```{r}
fdr = lisa_fdr(lisa, 0.05)
fdr
```

Then, one can set 0.05 as the cutoff p-value to filter the cluster results: 
```{r}
#lisa_clust = lisa_clusters(lisa, cutoff = fdr)
lisa_clust = lisa_clusters(lisa, cutoff = 0.05)
table(lisa_clust)

#0 Not significant
#1 High-High
#2 Low-Low
#3 High-Low
#4 Low-High
#5 Undefined
#6 Isolated
```

plot lisa output (clusters of correlation)
```{r}
lisa_pvalues = lisa_pvalues(lisa)
lisa_colors = lisa_colors(lisa)
#change white to blue for the non significant
lisa_colors[1] = "#47E5DA"
lisa_labels = lisa_labels(lisa)

plot(st_geometry(sfObj), 
     col=sapply(lisa_clust, function(x){return(lisa_colors[[x+1]])}), 
     border = "#333333", lwd=0.2)

title(main = "Local Moran Map")
legend('bottomleft', legend = lisa_labels, fill = lisa_colors, border = "#eeeeee")

p_labels = c("Not significant", "p <= 0.05", "p <= 0.01", "p <= 0.001")
p_colors = c("#47E5DA", "#7C47E5", "#479BE5", "#F34A4A")
plot(st_geometry(sfObj), 
     col=sapply(lisa_pvalues, function(x){
       if (x <= 0.001) return(p_colors[4])
       else if (x <= 0.01) return(p_colors[3])
       else if (x <= 0.05) return (p_colors[2])
       else return(p_colors[1])
       }), 
     border = "#333333", lwd=0.2)
title(main = "Local Moran Map")
legend('bottomleft', legend = p_labels, fill = p_colors, border = "#eeeeee")
```

## Only 2 CT c("Astrocyte", "Inhibitory")

```{r}
# convert ppp object to sf object
sfObj_2CT = st_as_sf(pp_2CT)

# Remove 1st row -> window info (otherwise crash)
sfObj_2CT = sfObj_2CT[-1,]

# change name of 1st column
colnames(sfObj_2CT)[1] = c("mark")

# convert marks to factors
sfObj_2CT$mark = as.factor(sfObj_2CT$mark)

# weights
knn_weights_sfObj_2CT = knn_weights(sfObj_2CT, k = sqrt(dim(sfObj_2CT)[1]) %>% floor) # 
```

Moran's test
```{r}
lisa_2CT = local_moran(knn_weights_sfObj_2CT, sfObj_2CT["mark"], cpu_threads = parallel::detectCores() * 0.75)
```

plot lisa_2CT output
```{r}
lisa_2CT_clust = lisa_clusters(lisa, cutoff = FALSE)
lisa_2CT_pvalues = lisa_pvalues(lisa_2CT)
lisa_2CT_colors = lisa_colors(lisa_2CT)
#change white to blue for the non significant
lisa_2CT_colors[1] = "#47E5DA"
lisa_2CT_labels = lisa_labels(lisa_2CT)

plot(st_geometry(sfObj_2CT), 
     col=sapply(lisa_2CT_clust, function(x){return(lisa_colors[[x+1]])}), 
     border = "#333333", lwd=0.2)

title(main = "Local Moran Map")
legend('bottomleft', legend = lisa_2CT_labels, fill = lisa_2CT_colors, border = "#eeeeee")

p_labels = c("Not significant", "p <= 0.05", "p <= 0.01", "p <= 0.001")
p_colors = c("#47E5DA", "#7C47E5", "#479BE5", "#F34A4A")
plot(st_geometry(sfObj_2CT), 
     col=sapply(lisa_2CT_pvalues, function(x){
       if (x <= 0.001) return(p_colors[4])
       else if (x <= 0.01) return(p_colors[3])
       else if (x <= 0.05) return (p_colors[2])
       else return(p_colors[1])
       }), 
     border = "#333333", lwd=0.2)
title(main = "Local Moran Map")
legend('bottomleft', legend = p_labels, fill = p_colors, border = "#eeeeee")
```

## Multivariate Local Geary test

-> Geary's C is inversely related to Moran's I, but it is not identical. Moran's I is a measure of global spatial autocorrelation, while Geary's C is **more sensitive to local spatial autocorrelation**. 

$$
C = \frac{(N = 1) \sum_{i}  \sum_{j} w_{ij} (x_i - x_j)²} {2W \sum_{i} (x_i - \overline{x})²}
$$

Where N is the number of spatial units indexed by i and j.
$x$ is the variable of interest, $\overline{x}$ is the mean of $x$ 
$Wij$ is a matrix of spatial weights with zeroes on the diagonal

-> The value of Geary's C lies between 0 and some unspecified value greater than 1. 
  + Values significantly lower than 1 demonstrate increasing positive spatial autocorrelation, 
  + Values significantly higher than 1 illustrate increasing negative spatial autocorrelation.

```{r}
lisa_geary = local_multigeary(knn_weights_sfObj, sfObj["mark"])

lisa_pvalues = lisa_pvalues(lisa_geary)
#change white to blue for the non significant
lisa_labels = lisa_labels(lisa_geary)

plot(st_geometry(sfObj), 
     col=sapply(lisa_clust, function(x){return(lisa_2CT_colors[[x+1]])}), 
     border = "#333333", lwd=0.2)

title(main = "Multivariate Local Geary")
legend('bottomleft', legend = lisa_labels, fill = lisa_2CT_colors, border = "#eeeeee")

p_labels = c("Not significant", "p <= 0.05", "p <= 0.01", "p <= 0.001")
p_colors = c("#47E5DA", "#7C47E5", "#479BE5", "#F34A4A")
plot(st_geometry(sfObj), 
     col=sapply(lisa_pvalues, function(x){
       if (x <= 0.001) return(p_colors[4])
       else if (x <= 0.01) return(p_colors[3])
       else if (x <= 0.05) return (p_colors[2])
       else return(p_colors[1])
       }), 
     border = "#333333", lwd=0.2)
title(main = "Multivariate Local Geary")
legend('bottomleft', legend = p_labels, fill = p_colors, border = "#eeeeee")
```

## Spatial Correlogram

-> A non-parametric spatial correlogram is an alternative measure of global spatial autocorrelation that **does not rely on the specification of a spatial weights matrix**. Instead, a local regression is fit to the covariances or correlations computed for all pairs of observations as a function of the distance between them.

-> If data is univariate, the spatial dependence is measured by Moran's I. If it is multivariate, it is measure by the centered **Mantel statistics**. The latter is used to calculate correlations between corresponding positions of 2 (dis)similarity or distance matrices.


-> With standardized variables z, this boils down to a local regression:

$$
z_i.z_j = f(d_{ij}) + \mu
$$

where $dij$ is the distance between a pair of locations i-j
$\mu$ is an error term
$f$ is the non-parametric function to be determined from the data (typically **LOWESS** or **kernel regression**)

## Spatial correlogram plot
-> The plot depicts how the spatial autocorrelation changes with distance.
  + The intersection between correlogram and the red line is the turning point from autocorrelation to spatial randomness
  
  
```{r}
## Adapted function from their github
plot.correlog = function(x, ...) {
##############################################################################
# this is the generic plot function for correlog objects
# significant values are represented by filled circles
##############################################################################
  args.default = list(xlab = "distance (mean-of-class)", ylab = "correlation", 
                       main = "Correlogram")
  args.input = list(...)
  args = c(args.default[!names(args.default) %in% names(args.input)], args.input)
  loess = loess(x$correlation ~ x$mean.of.class)
  do.call(plot, c(list(x = x$mean.of.class, y = x$correlation), args))
  
  lines(x$mean.of.class, x$correlation)
  lines(x$mean.of.class,  predict(loess), col = "blue",lwd = 3)
  abline(h=0, col = "red")
  if (!is.null(x$p)) {
      points(x$mean.of.class[x$p < 0.025], x$correlation[x$p < 0.025], pch = 21, 
             bg = "black")
  }
}
```

### Astrocyte and Inhibitory (Little correlation)

```{r}
#subset the data to only look at sample ID 0.01
sub_2CT = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte","Inhibitory")]
#sub = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte", "Pericytes")]
(pp_2CT = .ppp(sub_2CT, marks = "cluster_id"))
```

```{r, results='hide'}
sptl_correlogram = correlog(x = pp_2CT$x, y = pp_2CT$y, z = ifelse(pp_2CT$marks == "Astrocyte", 0 , 1), increment = 10, resamp = 50)
```

```{r}
#' {correlation}{the value for the Moran (or Mantel) similarity.}
#' {mean.of.class}{the actual average of the distances within each distance class.}
#' {nlok}{the number of pairs within each distance class.}
#' {x.intercept}{the interpolate x.intercept of Epperson (1993).}
#' {p}{the permutation p-value for each distance-class.}
#' {corr0}{If a cross-correlogram is calculated, corr0 gives the empirical within-patch cross-correlation.}

plot.correlog(sptl_correlogram) 
```

### OD Mature and Excitatory (Bigger correlation)

```{r}
#subset the data to only look at sample ID 0.01
sub_2CT = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("OD Mature","Excitatory")]
#sub = spe[, spe$sample_id == 0.01 & spe$cluster_id %in% c("Astrocyte", "Pericytes")]
(pp_2CT = .ppp(sub_2CT, marks = "cluster_id"))
```


```{r, results='hide'}
sptl_correlogram = correlog(x = pp_2CT$x, y = pp_2CT$y, z = ifelse(pp_2CT$marks == "OD Mature", 0 , 1), increment = 10, resamp = 50)
```

```{r}
#' {correlation}{the value for the Moran (or Mantel) similarity.}
#' {mean.of.class}{the actual average of the distances within each distance class.}
#' {nlok}{the number of pairs within each distance class.}
#' {x.intercept}{the interpolate x.intercept of Epperson (1993).}
#' {p}{the permutation p-value for each distance-class.}
#' {corr0}{If a cross-correlogram is calculated, corr0 gives the empirical within-patch cross-correlation.}

plot.correlog(sptl_correlogram) 
```

# Session info

```{r}
#| label: session-info
sessionInfo()
```
