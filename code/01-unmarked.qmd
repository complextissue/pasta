# Preamble

## Dependencies

```{r, cache=FALSE}
#| label: load-libs
#| message: false
#| warning: false
#| results: hide
source("utils.R")
```

## Setup

```{r, cache=FALSE}
#| label: load-data
spe <- readRDS("../data/spe.rds")

#subset the data to only look at sample ID 0.01
sub <- spe[, spe$sample_id == 0.01]
(pp <- .ppp(sub, marks = "cluster_id"))

#split the multitype point process into several single type processes
#fist, set the marks of the point process to be factors
marks(pp) <- factor(marks(pp))
ppls <- split(pp)
```

If not otherwise indicated, all information was taken from Baddeley et al. - Spatial Point Patterns.

```{r, fig.width=12, fig.height=12, cache=FALSE}
#Plot the entire point process where the marks are overlayed
plot(unmark(pp), main = 'Point Pattern Unmarked')
#Plot the marks separately 
plot(ppls, main = 'Point Pattern Marks Seperated')
```

## Concepts and Definitions of Point Processes

### Point Process

```{r}

```

### Complete Spatial Randomness

Complete spatial randomness (CSR) is the null model of point patterns, being the result of a poisson process. A completely random process is characterised by two properties

#### Homogeneity

Homogeneity means that the expected number of points falling into a given region $B$ is proportional to its area $|B|$ given a proportionality constant $\lambda$. The constant $\lambda$ is the intensity of the process, so the average number of points in a unit area.

$$
\mathbb{E}[X\cap B] = \lambda |B|
\label{eq:expected_number_points}
$$

#### Independence

Independence means that in two regions $A$ and $B$ the number of points $n(X\cap A)$ and $n(X\cap B)$ are two independent random variables. That means the number of points in region $A$ does not affect the number of points in region $B$. The number of points follow a poisson distribution:

$$
\mathbb{P}[N=k] = e^{-\mu}\frac{\mu^k}{k!}\\
\label{eq:poisson_process}
$$

### Inhomogeneous Poisson Process

A poisson process that is spatially varying in its average density of points is called inhomogeneous. Here, the average density $\lambda(u)$ is a function of spatial location $u$. The expected number of points falling into a region $B$ is 

$$
\mu = \int_{B} \lambda(u)du 
\label{eq:expected_number_inhomogeneous}
$$

### Stationarity

"A point process is called stationary if, when we view the process through a window $W$, its statistical properties do not depend on the location of the window in two-dimensional space."

### Isotropy

A point process is called isotropic, if its statistical properties are invariant to rotations. A CSR process is both stationary and isotropic.

### Intensity

Intensity is the expected density of points per unit area, as seen above. It can be interpreted as the rate of occurrence or the abundance of events recorded. The intensity itself is called a first moment property - being related to the expected number of points.

#### Estimating Intensity

The intensity can be estimated regardless of the type of the point pattern. In order to do so, we sum the individual intensities of the marks

```{r, cache=FALSE}
intensityPointProcess<- function(pp,mark){
  if(mark == TRUE){
    return(intensity(pp))
  }
  else{
    return(sum(intensity(pp)))
  }
}

intensityPointProcess(pp,mark=FALSE)
```

else we can look at each mark individually

```{r, cache=FALSE}
intensityPointProcess(pp,mark=TRUE)
```
#### Quadrat Counting

quadrat counting can again be performed for the entire picture

```{r, fig.width=12, fig.height=12, cache=FALSE}
Q5 <- quadratcount(pp, nx=5, ny=5)
plot(unmark(pp), main='Unmarked Point Pattern Quadrats')
plot(Q5, col='black', add=TRUE)
```

or for each type separately, treating each type as an unmarked process

```{r, fig.width=12, fig.height=12, cache=FALSE}
Q5sep <- quadratcount(ppls, nx=5, ny=5)
plot(Q5sep, main = 'Separated Marks Quadrats')
#plot(ppls, add=TRUE)
```

The quadrat counts can be tested against regularity. This can happen again in the unmarked pattern or in the separated types

```{r, cache=FALSE}
quadratTestPointProcess <- function(pp, mark){
  if(mark==TRUE){
    return(lapply(split(pp), quadrat.test, 5, alternative="regular", method="MonteCarlo"))
  }
  else{
    return(quadrat.test(unmark(pp), 5, alternative="regular", method="MonteCarlo"))
  }
}
quadratTestPointProcess(pp, mark=FALSE)
```
#### Kernel Estimation

```{r, fig.width=12, fig.height=12, cache=FALSE}
Dens <- density(pp)
plot(Dens, main = 'Kernel Density')
```

```{r, fig.width=12, fig.height=12, cache=FALSE}
Denssep <- density(ppls)
plot(Denssep, main = 'Kernel Density Separated Marks  ')
```

### Summarising Functionality for Estimating Intensity

```{r, eval=FALSE, cache=FALSE}
#PRE:
#POST:
estimatingIntensity <- function(
  pp
  method = c('expectation', 'quadrat', 'kernel', 'density', 'adaptive-density'),
  homogeneous = FALSE,
  nx = NULL,
  ny = NULL,
  correction = c(),
){

}
```

```{r, cache=FALSE}

```

### Testing for CSR

```{r, cache=FALSE}
#PRE: takes a point process and the indication, which test for CSR should be performed and potentially a covariate
#POST: returns if a point process or its individual point process marks are CSR or not.
#TODO: Change maybe to a switch statment in terms of computing time; Change lapply to mclapply later on
#TODO: There is a conceptual mistake - we pull out marks and test them against a markov simulation. It should rather be to test against all the other cells
testingCSR <- function(
    pp,
    method = c('quadrat','cdf','bermans','clark-evans','hopkins-skellam'),
    mark = FALSE,
    covariate = NULL,
    test = c('ks', 'cvm', 'ad'),
    verbose = FALSE
){
  #perform a quadrattest for individual marks or for the entire pointprocess
  if(method == 'quadrat'){
    if(mark == TRUE){
      test.result <- lapply(split(pp), quadrat.test, 5, method="MonteCarlo")
    }
    else{
      test.result <- quadrat.test(unmark(pp), 5, method="MonteCarlo")
    }
   }
  #perform a cdftest for individual marks or for the entire pointprocess given a covariate
  else if(method == 'cdf' && !is.null(covariate)){
    if(mark == TRUE){
      test.result <- lapply(split(pp), cdf.test, covariate, test=test)
    }
    else{
      test.result <- cdf.test(unmark(pp), covariate, test=test)
    }
  }
  #perform a bermans test for individual marks or for the entire pointprocess
  else if(method == 'bermans' && !is.null(covariate)){
     if(mark == TRUE){
      test.result <- lapply(split(pp), berman.test, covariate, test='Z1')
    }
    else{
      test.result <- berman.test(unmark(pp), covariate, test='Z1')
    } 
  }
  #perform a clark evans test for individual marks or for the entire pointprocess
  else if(method == 'clark-evans'){
     if(mark == TRUE){
      test.result <- lapply(split(pp), clarkevans.test)
    }
    else{
      test.result <- clarkevans.test(unmark(pp))
    } 
  }
  #perform a hopkins-skellam test for individual marks or for the entire pointprocess
  else if(method == 'hopkins-skellam'){
     if(mark == TRUE){
      test.result <- lapply(split(pp), hopskel.test)
    }
    else{
      test.result <- hopskel.test(unmark(pp))
    } 
  }
  #base case of the "switch" statement
  else{
    print("ERROR: non-specified arguments or methods")
    return(NULL)
  }
  
  #summarise the results as a mask of booleans to indicate which structures are 
  #random and which are not
  if(mark==TRUE){
    p.value.mask <- lapply(test.result, function(x) x$p.value>0.05)
  }
  else{
    p.value.mask <- test.result$p.value>0.05
  }
  #return the values of the test calculations, either just the boolean if 
  #CSR or not or the entire test statistics
  if(verbose == TRUE){
    return(test.result)
  }
  else{
    return(p.value.mask)
  }
}
result <- testingCSR(pp,method='clark-evans',mark=TRUE, verbose=FALSE)

testingCSR(ppls$Ependymal, method='clark-evans')
testingCSR(ppls$`OD Mature`, method='clark-evans')
testingCSR(ppls$Microglia, method='clark-evans')
```

## Correlation

Correlation or more generally covariance is called a second order quantity and measures dependence between data points. In point processes 

### Morisita Index

$$
M = m \frac{\sum_jn_j(n_j-1)}{n(n-1)}
$$

where we subdivide the point process into $m$ quadrats. The formula above is called hte Morisita index which is the ratio of observed and expected fractions. If the points are independent the Morisita index is close to 1, greater than 1 if they are clustered, and less than 1 if they are regular. This concept is closely linked to the index of dispersion (in fact with a bit of algebra, these two can be transformed into each other)

$$
I = \frac{s^2}{\bar{n}} = \frac{m}{n(m-1)}\sum_{j=1}^m\left(n_j-\frac{n}{m}\right)^2
$$

```{r, cache=FALSE}
miplot(ppls$Ependymal)
miplot(ppls$`OD Mature`)
miplot(ppls$Microglia)
```

### Fry plot

A fryplot is a scatterplot of the vector differences $x_j-x_i$ between all paris of distinct points in the pattern.

```{r, cache=FALSE}
fryplot(ppls$Ependymal)
fryplot(ppls$`OD Mature`)
fryplot(ppls$Microglia)
```

### Nearest Neighbour approaches

Nearest neighbour methods center around the notion of a nearness. In this section, we introduce `nndist`, a method to calculate the distances until $k$ nearest neighbours are found. This information is summarised as a histogram. In order to distill more information from this nearest-neighbour distance histogram, we follow the approach from the function `nnclean` and fit a Gamma mixture model to the data. The number of mixture components can be set individually. We choose a value of $k=2$ to model close range interactions and long range interactions. This gives us parameteric models for our histogram of nearest-neighbour distances. The aim would be to compare these parameteric models via statistical tests such as Anderson-Darling or Kolmogorov Smirnov. Another idea would be to compute the distance between these models with e.g. a Kullback-Leibler Divergence.

```{r, cache=FALSE}
# potentially easier to just calculate the distances to a nearest neighbour and plot the hist and fit a mixture model - difficult to set the number of components in that case 
# nndistances_k5_ependymal <- nndist(ppls$Ependymal, k = 5)
# nndistances_k5_odmature <- nndist(ppls$`OD Mature`, k = 5)
# nndistances_k5_microglia <- nndist(ppls$Microglia, k = 5)
# 
# #transform the distances to sqrt and try out normalisation by intensity
# nndistances_k5_ependymal <- sqrt(nndistances_k5_ependymal)#*intensity(ppls$Ependymal)#/median(sqrt(nndistances_k5_ependymal))
# nndistances_k5_odmature <- sqrt(nndistances_k5_odmature)#*intensity(ppls$`OD Mature`)#/median(sqrt(nndistances_k5_odmature))
# nndistances_k5_microglia <- sqrt(nndistances_k5_microglia)#*intensity(ppls$Microglia)#/median(sqrt(nndistances_k5_microglia))

nndistances_k5_ependymal<-sqrt(BiocNeighbors::findKNN(as.data.frame(ppls$Ependymal), k = 15)$distance)
nndistances_k5_odmature<-sqrt(BiocNeighbors::findKNN(as.data.frame(ppls$`OD Mature`), k = 15)$distance)
nndistances_k5_microglia<-sqrt(BiocNeighbors::findKNN(as.data.frame(ppls$Microglia), k = 15)$distance)

#calculate the row Median distance of k 1-10
nndistances_ependymal_median <- rowMedians((nndistances_k5_ependymal))
nndistances_odmature_median <- rowMedians((nndistances_k5_odmature))
nndistances_microglia_median <- rowMedians((nndistances_k5_microglia))

#create a histogram of the nearest-neighbour distances
hist_ependymal <- hist(nndistances_ependymal_median, breaks = 25, plot = FALSE)
hist_odmature <- hist(nndistances_odmature_median, breaks = 25, plot = FALSE)
hist_microglia <- hist(nndistances_microglia_median, breaks = 25, plot = FALSE)

#fit now a gamma mixture model to this nndist data
gamma_mm_comp2_ependymal <- mixR::mixfit(nndistances_ependymal_median, ncomp=2,family = 'gamma')
gamma_mm_comp2_odmature <- mixR::mixfit(nndistances_odmature_median, ncomp=2,family = 'gamma')
gamma_mm_comp2_microglia <- mixR::mixfit(nndistances_microglia_median, ncomp=2,family = 'gamma')

#plot this
c1 <- rgb(173,216,230,max = 255, alpha = 100, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 100, names = "lt.pink")
c3 <- rgb(144,238,144, max = 255, alpha = 100, names = "lt.green")

plot(hist_ependymal, col = c1, main = "Histogram of sqrt nn-distances with Gamma Mixtures fitted",ylim=c(0,1), freq=FALSE, xlab='Median Nearest neighbour distances sqrt transformed')
legend(20,1, legend=c("Ependymal", "OD Mature", "Microglia"),col=c('blue', 'red', 'green'), lty=1, cex=0.8)
#plot the probability density of the finite mixture model
lines(density(gamma_mm_comp2_ependymal),col='blue')
plot(hist_odmature, col = c2, add = TRUE, freq=FALSE)
lines(density(gamma_mm_comp2_odmature),col='red')
plot(hist_microglia, col = c3, add = TRUE, freq=FALSE)
lines(density(gamma_mm_comp2_microglia),col='green')

#kolmogorov smirnov test
ks.test(gamma_mm_comp2_ependymal$data,gamma_mm_comp2_odmature$data)
ks.test(gamma_mm_comp2_microglia$data,gamma_mm_comp2_odmature$data)
ks.test(gamma_mm_comp2_ependymal$data,gamma_mm_comp2_microglia$data)

kSamples::ad.test(gamma_mm_comp2_ependymal$data, gamma_mm_comp2_microglia$data, gamma_mm_comp2_odmature$data)
```

```{r, cache=FALSE}
create_ridge_plot <- function(nndistances, label){
  nndistances_df <- as.data.frame(nndistances)
  nndistances_df$index <- 1:nrow(nndistances_df)
  #add the row median
  nndistances_df$median <- rowMedians((nndistances))
  nndistances_df_melted <- reshape2::melt(nndistances_df, id = 'index')
  nndistances_df_melted <-dplyr::rename(nndistances_df_melted, K = variable)
  return(ggplot(nndistances_df_melted, aes(x = value, y = K, fill=K)) + theme(legend.position="none") + ggtitle(label) + ggridges::geom_density_ridges())
}

create_ridge_plot(nndistances_k5_ependymal, label='Ependymal')
create_ridge_plot(nndistances_k5_odmature, label='OD Mature')
create_ridge_plot(nndistances_k5_microglia, label='Microglia')
```

#### small simulation to test nndistance approach using 2D Gaussian

```{r, cache=FALSE}
set.seed(123)
create_multivariate_normal <- function(Sigma, mu, number){
  return (data <- MASS::mvrnorm(n = number, mu, Sigma))
}
n1 <- 1e3
n2 <- 1e3
data1 <- create_multivariate_normal(matrix(c(5,3,3,5),2,2),mu=c(1,1), number = n1)
data2 <- create_multivariate_normal(matrix(c(5,3,3,5),2,2),mu=c(1,1), number = n2)

w1 <- c(-10,10,-10,10)
w2 <- c(-10,10,-10,10)

data.pp1 <- as.ppp(data1,w1)
data.pp2 <- as.ppp(data2,w2)

plot(data.pp1)
plot(data.pp2)

#calculate full distance matrix of pairwise euclidean distances
# dist.matrix1 <- pairdist.default(data.pp1)
# dist.matrix2 <- pairdist.default(data.pp2)

#calculate the distances to the 15 nearest neighbour and take the sqrt of the distances
nndistances_pp1<-sqrt(BiocNeighbors::findKNN(as.data.frame(data.pp1), k = 10)$distance)
nndistances_pp2<-sqrt(BiocNeighbors::findKNN(as.data.frame(data.pp2), k = 10)$distance)

#create ridge plots
create_ridge_plot(nndistances_pp1, label = 'pp1')
create_ridge_plot(nndistances_pp2, label = 'pp2')

#calculate the row Median distance of k 1-10
nndistances_pp1_median <- rowMedians((nndistances_pp1))
nndistances_pp2_median <- rowMedians((nndistances_pp2))


#create a histogram of the nearest neighbour distances
hist_pp1 <- hist(nndistances_pp1_median, breaks = 50, plot = FALSE)
hist_pp2 <- hist(nndistances_pp2_median, breaks = 50, plot = FALSE)

#fit a gamma mixture model
gamma_mm_comp2_pp1 <- mixR::mixfit(nndistances_pp1_median, ncomp=2,family = 'gamma')
gamma_mm_comp2_pp2 <- mixR::mixfit(nndistances_pp2_median, ncomp=2,family = 'gamma')

#define the colour scheme
c1 <- rgb(173,216,230,max = 255, alpha = 100, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 100, names = "lt.pink")

plot(hist_pp1, col=c1, main = "Histogram of k=5 nn-distances with Gamma Mixtures fitted",freq=FALSE, ylim=c(0,6), xlab = "nn-distance")
legend(1.1,4, legend=c("pp1", "pp2"),col=c('blue', 'red'), lty=1, cex=0.8)
lines(density(gamma_mm_comp2_pp1),col='blue')
plot(hist_pp2, col=c2, n=25, add=TRUE, freq=FALSE)
lines(density(gamma_mm_comp2_pp2),col='red')

#calculate kolmogorov smirnov test on the two distributions
ks.test(gamma_mm_comp2_pp1$data,gamma_mm_comp2_pp2$data)
```


Simulation from Pierre-Luc and approach of having a matrix of k x p dimension so for k values 1-15 e.g. the nn distances across space. Then comparing the median nndistance over space in a ks test

```{r, eval = TRUE, cache=FALSE}
set.seed(123)
x1 <- rnorm(300,rep(10*sample.int(3,3),each=40),sd=5)
#x1 <- c(x1, rnorm(150,rep(10*sample.int(3,3),each=40),sd=10))
y1 <- rep(0,300)
#y1 <- rnorm(150,rep(10*sample.int(3,3),each=40),sd=3)
#y1 <- c(y1, rnorm(150,rep(10*sample.int(3,3),each=40),sd=10))

x2 <- rnorm(300,rep(10*sample.int(5,5),each=40),sd=5)
y2 <- rnorm(300,rep(10*sample.int(5,5),each=40),sd=5)


d1 <- cbind(x1,y1)
d2 <- cbind(x2,y2)

# distance to the 5th NN transformed via sqrt
nnd1 <- sqrt(BiocNeighbors::findKNN(d1, k = 15)$distance)
nnd2 <- sqrt(BiocNeighbors::findKNN(d2, k = 15)$distance)

d1 <- cbind(d1,nnd1)
d2 <- cbind(d2,nnd2)

#ridge plot to see dependence on k
create_ridge_plot(nnd1, label = 'Scenario A')
create_ridge_plot(nnd2, label = 'Scenario B')

nnd1_mean <- rowMeans((nnd1))
nnd2_mean <- rowMeans((nnd2))

d1 <- cbind(d1,nnd1_mean) %>% as.data.frame()
d2 <- cbind(d2,nnd2_mean) %>% as.data.frame()

# plot all
d <- data.frame(distance=c(nnd1_mean,nnd2_mean),
                scenario = c(rep('A', length(nnd1_mean)),
                             rep('B',length(nnd2_mean))))
cowplot::plot_grid(nrow=2,
  cowplot::plot_grid(nrow=1,
    ggplot(d1, aes(x1,y1, col = nnd1_mean)) + geom_point() + ggtitle("Scenario A") + labs(y='y', x = 'x', colour = "mean nndistance")+
     scale_colour_gradient(low = "red", high = "yellow", na.value = NA,limits=c(min(nnd1_mean,nnd2_mean),max(nnd1_mean,nnd2_mean))),
    ggplot(d2, aes(x2,y2, col = nnd2_mean)) + geom_point() + ggtitle("Scenario B")+ labs(y='y', x = 'x', colour = "mean nndistance")+
     scale_colour_gradient(low = "red", high = "yellow", na.value = NA,limits=c(min(nnd1_mean,nnd2_mean),max(nnd1_mean,nnd2_mean)))),
  ggplot(d, aes(distance, colour=scenario)) + geom_density() + xlab("Mean of NN distance k=1-15"))

#ks test on the two mean distributions
ks.test(nnd1_mean,nnd2_mean)
```

instead taking a different approach and sampling two curves from the k x p matrix and performing a 2D peacock test on this matrix for both groups

```{r, cache=FALSE}
set.seed(123)
#sub sample two columns via transpose at random from the dataframe (k vs. distances) of the curves 5:15
index_k <-  sample(5:ncol(nnd1), 2)
nnd1_sub <- nnd1[, index_k]
nnd2_sub <- nnd2[, index_k]

#eCDF of the sub sampled
plot(ecdf(nnd1_sub[,1]), main='Scenario A and B sub sampled', col = 'red', xlim = c(min(nnd1_sub),max(nnd1_sub)))
plot(ecdf(nnd1_sub[,2]), add=TRUE, col = 'red')
plot(ecdf(nnd2_sub[,1]), add=TRUE, col = 'blue')
plot(ecdf(nnd2_sub[,2]), add=TRUE, col = 'blue')

#fasano-franceschini test - faster version of the peacock test by taking the max of points within the sample - peacock does so over the set of all coordinate-wise combinations
fasano.franceschini.test::fasano.franceschini.test(nnd1_sub,nnd2_sub,nPermute=10000)
```

in another idea we can try to average the eCDFs we obtain and comparing the absolute difference between the resulting average eCDFs per group as done in the distinct paper by Simone Tiberi. In the end we compare this to a null distribution generated via permutation tests

```{r}
set.seed(123)
observed_teststatistic <- function(pp1, pp2, P){
  #eCDF median k kurves
  plot(ecdf(pp1), main='Scenario A and B mean curves', col = 'red', xlim = c(min(c(pp1,pp2)),max(pp1)))
  plot(ecdf(pp2), add=TRUE, col = 'blue')
  
  #calculate the eCDF of the two processes
  ecdf1_fun <- ecdf(pp1)
  ecdf2_fun <- ecdf(pp2)
  
  #calculate the sequence over which to discretise, has to be the min and max over all samples
  seq <- seq(from = min(c(pp1,pp2)), max(c(pp1,pp2)), length.out=P)
  #discretise the eCDFs such that we have comparable values for each vector - TODO: have to add the index here to the vector!
  ecdf1 <- unlist(lapply(seq, ecdf1_fun))
  
  ecdf2 <- unlist(lapply(seq, ecdf2_fun))
  
  plot(ecdf1, col = 'red')
  points(ecdf2, col = 'blue')
  #calculate the difference between the eCDFs how to handle if number of observations is not the same?
  ecdf.diffs <- abs(ecdf1 - ecdf2)
  
  #sum of the differences as the observed test statistic
  s_obs <- sum(ecdf.diffs)
  return(s_obs)
}

permutation.test <- function(s_obs,pp1,pp2,P,nsim){
  distribution=c()
  for(i in 1:nsim){
    #perform the permutation index which has the length of the combined processes
    perm <- sample(length(pp1)+length(pp2), replace=FALSE)
    #concatenate the samples to perform one large permutation
    pp_total <- c(pp1,pp2)
    #permute
    pp_total <- pp_total[perm]
    #get two samples of the original size again
    pp1_sample <- pp_total[1:length(pp1)]
    pp2_sample <- pp_total[(length(pp1)+1):length(pp_total)]

    #calculate the eCDF of the two processes
    ecdf1_fun <- ecdf(pp1_sample)
    ecdf2_fun <- ecdf(pp2_sample)
    
    #calculate the sequence over which to discretise, has to be the min and max over all samples
    seq <- seq(from = min(c(pp1,pp2)), max(c(pp1,pp2)), length.out=P)

    #discretise the eCDFs such that we have comparable values for each vector
    ecdf1 <- unlist(lapply(seq, ecdf1_fun))
    ecdf2 <- unlist(lapply(seq, ecdf2_fun))
    #calculate the absolute difference between the two patterns
    ecdf.diffs <- abs(ecdf1 - ecdf2)
    #aggreate the sum of the differences in our test statistic
    s_p <- sum(ecdf.diffs)
    distribution[i] = s_p
  }
  hist(distribution, breaks = 25)
  abline(v=s_obs,col="red")
  return(paste0('p-value permuted ', (sum(distribution>=s_obs)+1)/(nsim+1)))
}
s_obs <- observed_teststatistic(nnd1_mean,nnd2_mean, 400)
permutation.test(s_obs, nnd1_mean, nnd2_mean, 400, 10000)
```

in a third approach we can try to compare all 15 curves using an adapted version of Simones approach. We compute the abs difference between the pairwise k values for both scenarios and sum the test statistic for all of them. Then we compare that to a permutation based approach that was generated in the same way. 

```{r}
set.seed(123)
#the possible k values
k_ <- seq(1:15)
#the cumulative test observed statistic
s_obs <- 0
for(k in k_){
  #subset the data to the relevant column of k
  nnd1_sub <- nnd1[,k]
  nnd2_sub <- nnd2[,k]
  
  #calculate the eCDF of the two processes - this is a function in R
  ecdf1_fun <- ecdf(nnd1_sub)
  ecdf2_fun <- ecdf(nnd2_sub)
  
  #number of discrete steps
  P <- 100
  
  #calculate the sequence over which to discretise, has to be the min and max over all samples
  seq <- seq(from = min(c(nnd1_sub,nnd2_sub)), max(c(nnd1_sub,nnd2_sub)), length.out=P)
    
  #discretise the eCDFs such that we have comparable values for each vector - TODO: have to add the index here to the vector!
  ecdf1 <- unlist(lapply(seq, ecdf1_fun))
  ecdf2 <- unlist(lapply(seq, ecdf2_fun))
  
  #calculate the difference between the eCDFs how to handle if number of observations is not the same?
  ecdf.diffs <- abs(ecdf1 - ecdf2)

  #aggreate the sum of the differences in our test statistic
  s_obs <- s_obs + sum(ecdf.diffs)
}
#the observed test statistic aggregated over all values of k
s_obs

# adapted from https://towardsdatascience.com/permutation-test-in-r-77d551a9f891 and https://mac-theobio.github.io/QMEE/lectures/permutation_examples.notes.html

permutation.test.k <- function(s_obs,nsim){
  distribution=c()
  for(i in 1:nsim){
    #initialise test statistic
    s_p <- 0
    #permute the index of the two dataframes combined
    perm <- sample(nrow(nnd1) + nrow(nnd2))
    
    #concatenate the sampels
    nnd_conc <- rbind(nnd1,nnd2)
    #permute the actual dataframes
    nnd_conc <- nnd_conc[perm,]
    
    #take the dataframe apart
    nnd1 <- nnd_conc[1:nrow(nnd1),]
    nnd2 <- nnd_conc[(nrow(nnd1)+1):nrow(nnd_conc),]
    
    for(k in k_){
      #subset the data to the relevant column of k
      nnd1_sub <- nnd1[,k]
      nnd2_sub <- nnd2[,k]
      #calculate the eCDF of the two processes
      ecdf1_fun <- ecdf(nnd1_sub)
      ecdf2_fun <- ecdf(nnd2_sub)
      
      #number of discrete steps
      P <- 100
      
      #calculate the sequence over which to discretise, has to be the min and max over all samples
      seq <- seq(from = min(c(nnd1_sub,nnd2_sub)), max(c(nnd1_sub,nnd2_sub)), length.out=P)
      #discretise the eCDFs such that we have comparable values for each vector - TODO: have to add the index here to the vector!
      ecdf1 <- unlist(lapply(seq, ecdf1_fun))
      ecdf2 <- unlist(lapply(seq, ecdf2_fun))
      
      #calculate the difference between the eCDFs how to handle if number of observations is not the same?
      ecdf.diffs <- abs(ecdf1 - ecdf2)
      #aggreate the sum of the differences in our tst statistic
      s_p <- s_p + sum(ecdf.diffs)
    }
    distribution[i] = s_p
  }
  hist(distribution)
  abline(v=s_obs,col="red")
  return((sum(distribution>=s_obs)+1)/(nsim+1))
}

permutation.test.k(s_obs, 10000)
```

Further more we can just apply a $k$-sample version of the Anderson-Darling test of goodness of fit

```{r}
kSamples::ad.test(nnd1_mean, nnd2_mean)
``` 

```{r, cache=FALSE}
#trying to center around the mean
nnd1 <- nnd1 - nnd1_mean
nnd2 <- nnd2 - nnd2_mean

create_ridge_plot(nnd1, label = 'Scenario A')
create_ridge_plot(nnd2, label = 'Scenario B')
```

### Ripley's $K$

#### Empirical Ripley's $K$

In the framework of correlation analysis we often look at distances $d_{ij} = ||x_i-x_j||$ of all ordered points. It is a natural idea to look at the summary of these distances $d_{ij}$, e.g. a histogram. The histogram of this point process is a difficult statistic, as it depends on the shape and size of the observation window, thus the histogram can change significantly. Let's instead look at the empirical distribution funciton of the distances $d_{ij}$ that are smaller or equal than a radius $r$

$$
\hat{H}(r) = \frac{1}{n(n-1)}\sum_{i=1}^n \sum_{j=1\\j\neq i}^n \{d_{ij}\leq r\}
$$
This empirical cumulative distribution function is closely linked to the Morisita index in that both capture the fraction of pairs of points that lie close together. The similarity comes from the idea of "closeness" which is defined in $\hat{H}(r)$ by $r$ and in the Morisita index by the size of the quadrats. 

The contribution of each point $x_i$ to the sum above is

$$
t_i(r) = \sum_{j \neq i} \mathbb{1} \{d_{ij}\leq r\}
$$

this number $t_i(r)$ is the number of points that fall within a radius $r$ centered at $x_i$. It follows then:

$$
\hat{H}(r) = \frac{1}{n(n-1)}\sum_{i=1}^n t_i(r) = \frac{1}{n-1} \bar{t}(r)
$$

Here, we see what we actually want to measure is "the average number of r-neighbours of a typical random point". This number is still dependent on the size of the observation window so we want to standardise is by the number of points and $|W|$ the window size. Then we obtain the empirical Ripley's $K$ function

$$
\hat{K}(r) = \frac{|W|}{n(n-1)}\sum_{i=1}^n\sum_{j=1 \\j \neq i}^n\{d_{ij}\leq r\}
$$

The standardisation make it possible to compare point patterns with different observation windows and with different numbers of points. Using the empirical $K$ function assumes though tha the point process has homogeneous intensity. 

We can add bootstrapped confidence intervals

```{r, cache=FALSE}
K_ependymal <- lohboot(ppls$Ependymal, fun='Kest')
K_ependymal$type <- 'Ependymal'

K_odmature <- lohboot(ppls$`OD Mature`, fun='Kest')
K_odmature$type <- 'OD Mature'

K_microglia <- lohboot(ppls$Microglia, fun='Kest')
K_microglia$type <- 'Microglia'

#create a list that combines bove values
K_list <- rbind(K_ependymal, K_odmature, K_microglia)

p <- ggplot(K_list, aes(x=r, y=iso, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('estimated K-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```

#### True $K$-function

instead of the summary of pairwise distances, we are interested in the point process. In order to do so, we have to think about the expected number of $r$-neighbours given a point $X$ at a location $u$ divided by its intensity $\lambda$

This means

$$
K(r) = \frac{1}{\lambda} \mathbb{E} [t(u,r,X)|u \in X]
$$

where

$$
t(u,r,X) = \sum_{j=1}^{n(X)} \mathbb{1} \{0<||u-x_j||\leq r\}
$$

This definition of the true $K$ function is only valid if the point process is stationary. For a homogeneous poisson process we obtain

$$
K_{pois}(r) = \pi r^2
$$

in our cases we don't find a homogeneous point process

#### Edge corrections for the $K$-functions

There are many corrections for edge effects. They are briefly listed here

##### Border correction

In border correction the summation of data points is restricted to $x_i$ for which $b(x_i,r)$ is completely in the window $W$.

##### Isotropic correction

We can regard edge effect as a sampling bias. Larger distances (e.g. close to the edges) are less likely to be observed. This can be corrected for.

##### Translation correction

A stationary point process $X$ is invariant to translations. So the entire point process can be shifted by a vector $s$ to be at the position $X+s$. 

### $L$-function

The $K$-function can be centered which is then called the $L$-function

$$
L(r) = \sqrt{\frac{K(r)}{\pi}}
$$

We can add bootstrapped confidence intervals

```{r, cache=FALSE}
L_ependymal <- lohboot(ppls$Ependymal, fun='Lest')
L_ependymal$type <- 'Ependymal'

L_odmature <- lohboot(ppls$`OD Mature`, fun='Lest')
L_odmature$type <- 'OD Mature'

L_microglia <- lohboot(ppls$Microglia, fun='Lest')
L_microglia$type <- 'Microglia'

#create a list that combines bove values
L_list <- rbind(L_ependymal, L_odmature, L_microglia)

p <- ggplot(L_list, aes(x=r, y=iso, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('estimated L-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```

### Pair Correlation function

We have seen above, that the $K$-function is cumulative in nature. Meaning that the contributions of all distances smaller equal to $r$ are counted. An alternative is to take the derivative of the $K$-function in order to obtain contributions of distances between points equal to $r$. 

$$
g(r) = \frac{K'(r)}{2\pi r}
$$

"$g(r)$ is the probability of observing a pair of point of the process separated by a distance $r$, divided by the corresponding probability for a Poisson process."

#### Estimator of the pair correlation function

The pair correlation function can be estimated via kernel smoothing. In very large datasets the pair correlation function can be approximated using histogram-based methods. 

$$
\hat{g}(r) = \frac{|W|}{2 \pi r n (n-1)} \sum_{i=1}^n\sum_{j=1 \\j \neq i}^n \kappa_h(r-d_{ij})e_{ij}(r)
$$

where $\kappa$ is the smoothing kernel. $\kappa_h(x)$ is a rescaled version of the template kernel $\kappa$

$$
\kappa_h(x) = \frac{1}{h}\kappa\left(\frac{x}{h}\right)
$$

In the above, $\kappa$ can be any probability density "over the real line with mean 0". Usually, the Epanechinikov kernel is used as smoothing kernel with half-width $w$. 


```{r, cache=FALSE}
g_ependymal <- lohboot(ppls$Ependymal, fun='pcf')
g_ependymal$type <- 'Ependymal'

g_odmature <- lohboot(ppls$`OD Mature`, fun='pcf')
g_odmature$type <- 'OD Mature'

g_microglia <- lohboot(ppls$Microglia, fun='pcf')
g_microglia$type <- 'Microglia'

#create a list that combines bove values
g_list <- rbind(g_ependymal, g_odmature, g_microglia)

p <- ggplot(g_list, aes(x=r, y=border, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('inhomogeneous pcf-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```


### Correcting for Inhomogeneity

#### Inhomogeneous $K$-function

In the case that a spatial pattern is known or suspected to be inhomogeneous, we have to take this into account for our analysis. In the sample that we have chosen, Ependymal cells and OD mature cells are inhomogeneous by eye. Inhomogeneous alternatives can be calculated

$$
K_{inhom}(r) = \mathbb{E} \left[\sum_{x_j \in X} \frac{1}{\lambda(x_j)}\mathbb{1}\{0<||u-x_j||\leq r\}|u \in X\right]
$$

This theoretical quantity can be approximated with estimators such as

$$
\hat{K}_{inhom}(r) = \frac{1}{D^p|W|}\sum_i\sum_{j \neq i} \frac{\mathbb{1}\{||u-x_j||\leq r\}}{\hat{\lambda}(x_j)\hat{\lambda}(x_i)}e(x_j,x_i;r)
$$

where $e(u,v;r)$ is an edge correction weight, $\hat{\lambda}(u)$ is an estimator of the intensity of $u$ and $D^p$ is the pth power of 

$$
D = \frac{1}{|W|}\sum_i \frac{1}{\hat{\lambda}(x_i)}
$$

```{r, cache=FALSE}
K_ependymal_inhom <- lohboot(ppls$Ependymal, fun='Kinhom')
K_ependymal_inhom$type <- 'Ependymal'

K_odmature_inhom <- lohboot(ppls$`OD Mature`, fun='Kinhom')
K_odmature_inhom$type <- 'OD Mature'

K_microglia_inhom <- lohboot(ppls$Microglia, fun='Kinhom')
K_microglia_inhom$type <- 'Microglia'

#create a list that combines bove values
K_list_inhom <- rbind(K_ependymal_inhom, K_odmature_inhom, K_microglia_inhom)

p <- ggplot(K_list_inhom, aes(x=r, y=iso, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('inhomogeneous K-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```

#### Inhomogeneous $L$-function

```{r, cache=FALSE}
L_ependymal_inhom <- lohboot(ppls$Ependymal, fun='Linhom')
L_ependymal_inhom$type <- 'Ependymal'

L_odmature_inhom <- lohboot(ppls$`OD Mature`, fun='Linhom')
L_odmature_inhom$type <- 'OD Mature'

L_microglia_inhom <- lohboot(ppls$Microglia, fun='Linhom')
L_microglia_inhom$type <- 'Microglia'

#create a list that combines bove values
L_list_inhom <- rbind(L_ependymal_inhom, L_odmature_inhom, L_microglia_inhom)

p <- ggplot(L_list_inhom, aes(x=r, y=iso, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('inhomogeneous L-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```

#### Inhomogeneous pair-correlation function

```{r, cache=FALSE}
g_ependymal_inhom <- lohboot(ppls$Ependymal, fun='pcfinhom')
g_ependymal_inhom$type <- 'Ependymal'

g_odmature_inhom <- lohboot(ppls$`OD Mature`, fun='pcfinhom')
g_odmature_inhom$type <- 'OD Mature'

g_microglia_inhom <- lohboot(ppls$Microglia, fun='pcfinhom')
g_microglia_inhom$type <- 'Microglia'

#create a list that combines bove values
g_list_inhom <- rbind(g_ependymal_inhom, g_odmature_inhom, g_microglia_inhom)

p <- ggplot(g_list_inhom, aes(x=r, y=border, col= type))+
  geom_line()+
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
  ggtitle('inhomogeneous pcf-function')+
  geom_line(aes(x=r,y=theo),col = 'black',linetype = "dashed")+
  theme_light()
p
```

#### Locally-scaled $K$-function

In the inhomogeneous K approach above we assume that the local scale of the point process is not changed. The intensity is spatially varying. In a biological sample this assumption is easily violated, thinking e.g. of a gradient of cells that increases from one side to another. Therefore, we can assume that the process is subdiveded in small regions. In these small regions the point process is a scaled version of a template process. This template process has to be both stationary and isotropic. For two locations $u$ and $v$ we would then assume that 

$$
g(u,v) = g_1 \left(\frac{||u-v||}{s}\right)
$$

In this example, $g_1$ is the pair correlation function of the template process

#### DBSCAN

could be useful to detect spatial patterns and clusters

```{r, cache=FALSE}
pp_df <- as.data.frame(ppls$`OD Mature`)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =50, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)

pp_df <- as.data.frame(ppls$Ependymal)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =75, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)

pp_df <- as.data.frame(ppls$Microglia)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =100, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)

pp_df <- as.data.frame(ppls$Inhibitory)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =60, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)

pp_df <- as.data.frame(ppls$Excitatory)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =75, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)

pp_df <- as.data.frame(ppls$Pericytes)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =50, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)
```

# Appendix

## Session info

```{r, cache=FALSE}
#| label: session-info
sessionInfo()
```
